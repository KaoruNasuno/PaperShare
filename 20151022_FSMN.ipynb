{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文輪読\n",
    "那須野薫(Kaoru Nasuno)/ 東京大学松尾研究室(Matsuo Lab., the University of Tokyo)\n",
    "\n",
    "##メタ情報\n",
    "タイトル：Feedforward Sequential Memory Neural Networks without Recurrent Feedback  \n",
    "著者：Shiliang Zhang, Hui Jiang, Si Wei, Lirong Dai  \n",
    "公開：2015年10月  \n",
    "リンク： [arXiv](http://arxiv.org/abs/1510.02693)  \n",
    "ページ数：4  \n",
    "\n",
    "\n",
    "\n",
    "## Abstract\n",
    "Feedforward Sequential Memory Networks(FSMN)という新しい構造のメモリネットを提案する。  \n",
    "このメモリネットはrecurrent feedbackを利用しないで長期依存を学習することができる。   \n",
    "FSMNは隠れ層に学習可能なメモリブロックがついている標準的なFeedforward neural networks(FNN)である。  \n",
    "この論文では、FSMNをいくつかの言語モデリングのタスクに適用させた。  \n",
    "実験では、FSMNのメモリブロックは長期履歴の表現を効率的に学習できることが示された。  \n",
    "また、FSMNに基づいた言語モデルは他のFNNに基づいた言語モデルだけでなく、RNNに基づいた言語モデルも圧倒した。  \n",
    "\n",
    "### 用語\n",
    "メモリネット：メモリに相当する機構を持っているNN。  \n",
    "recurrent feedback：RNNの時系列方向のback propagation。  \n",
    "長期依存：時系列解析では短期依存の学習は容易。一方で長期依存を学習させることが難しく課題となっている。  \n",
    "メモリブロック：メモリに相当する機構。  \n",
    "言語モデリング：文章を学習対象とし、次の単語を予測するというモデリング。  \n",
    "RNN：言語モデリングの領域でstate of the artのNN。LSTM-RNNが長期依存をよく学習できることが分かっている。  \n",
    "\n",
    "\n",
    "\n",
    "##選択理由\n",
    "#### ここずっと時系列解析をやっている。\n",
    "- Kaggleやプロジェクト。\n",
    "- RNNはちゃんと学習させるの大変。学習時間とか，その他もろもろ。\n",
    "\n",
    "\n",
    "\n",
    "#### RNNではなくFNNで系列をよく捉えられる方法をを提案している。\n",
    "- 紹介する論文は数式の理解と実装が簡単。  \n",
    "- 精度がちゃんと出ている模様。\n",
    "\n",
    "\n",
    "\n",
    "## 感想\n",
    "実装が意外とだるい。  \n",
    "精度まぁまぁ、引き続き検証。\n",
    "\n",
    "## 目次\n",
    "- 背景・イントロ\n",
    "- FSMN\n",
    "- 実装\n",
    "- 実験\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景・イントロ\n",
    "4ページだけなので、細かいことを知りたい方は全部読んでください。  \n",
    "\n",
    "テキスト、会話、動画などの系列データのモデリングを機械学習で行うとき、長期依存を考慮することがとても大事。  \n",
    "伝統的なアプローチは、RNNやLSTM-RNNのrecurrent feedbacksで系列の長期依存構造を捉えようとするもの。   \n",
    "RNNは系列処理で人気。  \n",
    "特に最近では、RNNではなく明示的なメモリをもつneural computing modelも注目されている。  \n",
    "ここでは、4つ引用しており(いずれも2014〜)、  \n",
    "例えば、読み書きできるメモリブロックを持つもの(End-To-End Memory Networks)、   \n",
    "外部メモリへの接続を行うもの(Neural Turing Machine)がある。  \n",
    "\n",
    "この論文では、recurrent feedbackを使わないで長期依存を学習できるシンプルなメモリネット(FSMN)を提案している。  \n",
    "隠れ層にメモリブロックを導入して標準的なFNNを拡張する。  \n",
    "RNNとは異なり、最終的なFSMNの構造は単純なfeedward structureと同じで、RNNやLSTM-RNNより学習はより効率的に安定して行える。  \n",
    "FSMNを言語モデリング(LM)の2つのデータセットに適用して、FNN-LMsだけでなく、RNN-LMsも圧倒した。  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedfoward Sequential Memory Networks(FSMN)  \n",
    "<img src='files/FSMN.png' width=\"900px\"/>\n",
    "### 数式\n",
    "$$\n",
    "{\\bf X} = \\{{\\bf x}_1 , {\\bf x}_2 , \\ldots , {\\bf x}_T\\}\n",
    "$$  \n",
    "$$\n",
    "{\\bf H} = \\{{\\bf h}_1, {\\bf h}_2, \\ldots , {\\bf h}_T\\}\n",
    "$$  \n",
    "$$\n",
    "{\\bf \\tilde{h}}_t = f(\\sum^{N}_{i=0}a_i \\cdot {\\bf h}_{t-1})\n",
    "$$  \n",
    "$$\n",
    "{\\bf a} = \\{a_0 , a_1 , a_2 , \\ldots, a_N \\}\n",
    "$$  \n",
    "$$\n",
    "{\\bf M} = \\left[\n",
    "    \\begin{array}{rrrrrrr}\n",
    "      a_0 & a_1 & \\ldots & a_N & 0 & \\ldots & 0 &\\ldots \\\\\n",
    "      0 & a_0 &  a_1 & \\ldots & a_N & 0 & \\ldots & \\\\\n",
    "     &&&&&&& \\\\\n",
    "      \\vdots & \\ldots & 0 &  & \\ddots & \\ddots & & \\vdots  \\\\\n",
    "      0 & \\ldots & & & a_0 & & \\ldots & a_N \\\\\n",
    "      &&&&&&& \\\\\n",
    "      &&&&&&& \\\\\n",
    "      \\vdots & & \\ldots  &  & \\ddots & \\ddots & & \\vdots  \\\\\n",
    "      0  & \\ldots & & & & &  0 & a_0\n",
    "    \\end{array}\n",
    "  \\right]\n",
    "$$  \n",
    "$$\n",
    "{\\bf \\tilde{H}} = f({\\bf H} \\cdot {\\bf M})\n",
    "$$  \n",
    "\n",
    "${\\bf X}$：入力行列(系列順に並んでいる)  \n",
    "${\\bf H}$：隠れ層行列  \n",
    "$T$：${\\bf X}$の行数(系列数)  \n",
    "${\\bf h}_i$：隠れ層行列の$i$行  \n",
    "${\\bf M}$：メモリを計算するための行列    \n",
    "${\\bf H}$：隠れ層のメモリ行列  \n",
    "\n",
    "例えば、\n",
    "$$\n",
    "{\\bf H}_{l+1} = \\phi({\\bf H}_{l} \\cdot {\\bf W}_1 + {\\bf \\tilde{H}}_{l} \\cdot {\\bf W}_2 + b)\n",
    "$$  \n",
    "\n",
    "文章等、$K$個の系列でそれぞれ系列長が異なる場合、\n",
    "$$\n",
    "{\\bf \\tilde{H}}= f([{\\bf H_1}, {\\bf H_2}, \\ldots ,{\\bf H_K} ] \\cdot\n",
    "\\left[\n",
    "    \\begin{array}{rrrr}\n",
    "    {\\bf M_1} & & &\\\\\n",
    "    & {\\bf M_2} & & \\\\\n",
    "    & & \\ddots & \\\\\n",
    "    & & & {\\bf M_K} \\\\\n",
    "    \\end{array}\n",
    "  \\right]\n",
    ")\n",
    "= f({\\bf \\bar{H}} \\cdot {\\bf \\bar{M}})\n",
    "$$  \n",
    "のようにすればいいと書いてある。  \n",
    "しかし、実際には、これはいけていないさそうである。  \n",
    "例えば、平均系列長が$100$で系列数が$100,000$だと$100 \\times 100,000$と、  \n",
    "${\\bf \\tilde{H}}$は行数が1000万ぐらいになってしまい、計算が厳しそうである。  \n",
    "バッチごとに行列を作るか用意しておくか、するのが妥当なやりかただろうか。。。  \n",
    "\n",
    "### 解釈\n",
    "信号処理の観点では、  \n",
    "FSMNのメモリブロックは高次有限インパルス応答(high-order finite impuse response(FIR))フィルタとみなせるが、  \n",
    "RNNのrecurrent layer、つまり$ \\tilde{{\\bf h}}_t = f({\\bf h}_t + {\\bf W} \\cdot \\tilde{{\\bf h}}_{t-1})$は一次無限インパルス応答(first-order infinite impulse reponse(IIR))フィルタとみなせる。  \n",
    "明らかに、${\\bf a}はN番目のFIRフィルタの係数とみなせる。  \n",
    "(一般的に、おそらく信号処理の領域では？)、IIRフィルタはFIRフィルタよりもコンパクトだが、  \n",
    "IIRフィルタは実装が難しいかもしれない。  \n",
    "場合によってはIIRフィルタは不安定になるが、FIRは安定している。  \n",
    "さらに、IIRフィルタのようなRNNの学習には、計算の複雑さや勾配消滅や勾配爆発という問題を抱えているBPTTが必要だが、  \n",
    "一方で、提案するFIRフィルタのようなFSMNは標準的なBPの手続きで効率的に学習できる。  \n",
    "したがって、FSMNの学習はRNNより安定していて、効率的である。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装\n",
    "theanoで実装した。  \n",
    "特に実装が面倒だったのが${\\bf M}$の部分。  \n",
    "learnable parameterが一部で、かつ複数部分が同じパラメタなので。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "def relu(x):\n",
    "    return T.switch(T.gt(x, 0), x, 0.0)\n",
    "\n",
    "def step_fsmn_M(i, a, n_hists, n_hidden_back):\n",
    "    #ステップ関数\n",
    "    n_rep = T.minimum(n_hists-i, n_hidden_back)\n",
    "    m = T.set_subtensor(T.zeros((n_hists, ), dtype='float32')[i: i+n_rep], a[:n_rep])\n",
    "    return m\n",
    "\n",
    "def get_fsmn_M(a, n_hists, n_hidden_back):\n",
    "    # scanで行ごとに定義・置換していく。\n",
    "    M, _ = theano.scan(\n",
    "        fn=step_fsmn_M,\n",
    "        sequences=T.arange(n_hists),\n",
    "        outputs_info=None,\n",
    "        non_sequences=[a, n_hists, n_hidden_back],\n",
    "    )\n",
    "    return M\n",
    "\n",
    "def get_init_weight(rng, n_in, n_out):\n",
    "    return rng.uniform(\n",
    "        low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "        high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "        size=(n_in, n_out),\n",
    "    ).astype('float32')\n",
    "\n",
    "\n",
    "n_x = 100  # FNNへの入力の次元\n",
    "n_y = 10  # FNNの出力の次元\n",
    "n_hidden_mlp = 100  # 隠れ層の次元\n",
    "n_hidden_back = 10   # 何個前まで考えるか。\n",
    "n_hists = 1000  #  FNNへの入力の行数\n",
    "\n",
    "rng = numpy.random.RandomState(1234)  # Random Seed\n",
    "X = T.fmatrix('X')\n",
    "Y = T.fmatrix('Y')\n",
    "\n",
    "params = []\n",
    "\n",
    "\n",
    "W1 = theano.shared(get_init_weight(rng, n_x, n_hidden_mlp), name='W1')\n",
    "b1 = theano.shared(numpy.zeros((n_hidden_mlp,), dtype='float32'), name='b1')\n",
    "fsmn_a1m = theano.shared(numpy.ones((n_hidden_back,), dtype='float32') / n_hidden_back, name='fsmn_a1m')\n",
    "\n",
    "W2 = theano.shared(get_init_weight(rng, n_hidden_mlp, n_hidden_mlp), name='W2')\n",
    "W2m = theano.shared(get_init_weight(rng, n_hidden_mlp, n_hidden_mlp), name='W2m')\n",
    "b2 = theano.shared(numpy.zeros((n_hidden_mlp,), dtype='float32'), name='b2')\n",
    "\n",
    "\n",
    "W3 = theano.shared(get_init_weight(rng, n_hidden_mlp, n_y), name='W3')\n",
    "b3 = theano.shared(numpy.zeros((n_y,), dtype='float32'), name='b3')\n",
    "\n",
    "params += [W1, b1]\n",
    "params += [fsmn_a1m]\n",
    "params += [W2, W2m, b2]\n",
    "params += [W3, b3]\n",
    "\n",
    "H1 = relu(T.dot(X, W1) + b1)  # 1層目のアフィン変換\n",
    "\n",
    "M1 = get_fsmn_M(fsmn_a1m, n_hists, n_hidden_back)\n",
    "H1m = relu(T.dot(H1.T, M1).T)  # 1層目のメモリ\n",
    "\n",
    "H2 = relu(T.dot(H1, W2) + T.dot(H1m, W2m) + b2)  # 2層目のアフィン変換\n",
    "\n",
    "Y_pred = relu(T.dot(H2, W3) + b3)  # 最後の予測\n",
    "\n",
    "cost = T.sum((Y - Y_pred) ** 2)  # cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験\n",
    "隠れ層の活性化関数にはReluを、  \n",
    "初期化はXavier。\n",
    "\n",
    "<img src='files/FSMN_EXP.png' width=\"900px\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
