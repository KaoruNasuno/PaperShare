{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文輪読\n",
    "那須野薫(Kaoru Nasuno)/ 東京大学松尾研究室(Matsuo Lab., the University of Tokyo)\n",
    "\n",
    "##メタ情報\n",
    "タイトル：Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation  \n",
    "著者：Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y.   \n",
    "公開：2014  \n",
    "リンク： [arXiv](http://arxiv.org/abs/1406.1078)  \n",
    "被引用件数：84  \n",
    "\n",
    "## Abstract\n",
    "この論文では、2つのRNNからなるRNN Encoder-Decoderという新しいニューラルネットワークを提案する。  \n",
    "ひとつのRNNがシンボルのシーケンスを固定長のベクトル表現にエンコードし、もうひとつがその表現をシンボルのシーケンスにデコードするというもの。  \n",
    "提案モデルのエンコーダとデコーダはソースシーケンスをgivenとしたときのターゲットシーケンスの条件付き確率を最大化するようにひとつのネットワークとして学習させる。  \n",
    "既存のlog-linearモデルの素性にRNN Encoder-Decoderにより算出されたフレーズペアの条件付き確率を利用することで、統計的機械翻訳システムの性能が向上することが実験的に示される。  \n",
    "定性的には、提案モデルが言語フレーズの意味的・文法的に意味のある表現を学習することを示す。\n",
    "\n",
    "\n",
    "##選択理由\n",
    "#### Gated Recurrent Unitが気になっていた。\n",
    "- 最近RNNをいじっている。\n",
    "- LSTMよりマトリックス変数が少ないが精度が同程度？らしい。\n",
    "- 数式の理解と実装が簡単に見える。\n",
    "\n",
    "#### 可変長データを固定長に直し、さらに可変長にする方法を提案している。\n",
    "\n",
    "#### 統計的機械翻訳に特別強い興味がある、というわけではない。\n",
    "\n",
    "\n",
    "## 目次\n",
    "- RNN\n",
    "- RNN Encoder-Decoder\n",
    "- LSTM\n",
    "- GRU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "<img src='files/RNN2.png' width=\"500px\"/>\n",
    "$$h_{<t>} = f(h_{<t-1>}, x_t)$$  \n",
    "\n",
    "例えば、  \n",
    "$$h_{<t>} = tanh(W_xx_t + W_hh_{<t-1>})$$  \n",
    "$W_h, W_x$：パラメタ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "rng = numpy.random.RandomState(1234)\n",
    "x = T.fmatrix('x')\n",
    "n_x = 10\n",
    "n_hidden_e =10\n",
    "def get_init_weight(n_in, n_out):\n",
    "    return rng.uniform(\n",
    "        low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "        high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "        size=(n_in, n_out),\n",
    "    ).astype('float32')\n",
    "    \n",
    "Wh = theano.shared(\n",
    "    get_init_weight(n_hidden_e, n_hidden_e),\n",
    "    name='Wh'\n",
    ")\n",
    "\n",
    "Wx = theano.shared(\n",
    "    get_init_weight(n_x, n_hidden_e),\n",
    "    name='Wx'\n",
    ")\n",
    "\n",
    "def step_e(x, h_tm1, Wx, Wh):\n",
    "    return T.tanh(T.dot(x, Wx) + T.dot(h_tm1, Wh))\n",
    "\n",
    "h_t, updates = theano.scan(\n",
    "    fn=step_e,\n",
    "    sequences=x,\n",
    "    outputs_info=numpy.zeros(n_hidden_e).astype('float32'),\n",
    "    non_sequences=[Wx, Wh]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Encoder-Decoder\n",
    "<img src='files/RNN_Encoder-Decoder.png' width=\"400px\"/>\n",
    "$$論文より引用$$\n",
    "任意の長さのデータを固定長に変換して、さらに任意の長さに変換するRNN。  \n",
    "終了記号(=$x_T$)を読み込んだあとのhidden stateが全体の入力のサマリー(=$c$)になっている。  \n",
    "エンコーダ部分は普通のRNNと同じ。  \n",
    "デコーダ部分は$y_t$も$h'_{<t>}$も$y_{t-1}$の条件づけられる。  \n",
    "$$h'_{<t>}=f(h'_{<t-1>}, y_{t-1}, c)$$  \n",
    "$$y_{t}=f(h'_{<t>}, y_{t-1}, c)$$  \n",
    "学習では下記の$\\theta$を求める。  \n",
    "$$\\max_{\\theta}\\frac{1}{N}\\sum_{n=1}^Nlogp_{\\theta}(y_n|x_n)$$\n",
    "\n",
    "具体例としては、下記のものが考えられる。  \n",
    "(実際には、GRUと組み合わせているので、下記とは異なる。)\n",
    "$$c = tanh(Vh_{T})$$  \n",
    "$$h'_{<t>} = tanh(U_{hh}h'_{<t-1>} + U_{yh}y_{t-1} + U_{ch}c)$$  \n",
    "$$y_t = tanh(U_{hy}h'_{<t>} + U_{yy}y_{t-1} + U_{cy}c)$$  \n",
    "$V, U_{hh}, U_{yh}, U_{ch}, U_{hy}, U_{yy}, U_{cy}$：パラメタ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = T.fvector('y')\n",
    "n_hidden_d = 10\n",
    "n_c = 10\n",
    "n_y = 10\n",
    "V = theano.shared(\n",
    "    get_init_weight(n_hidden_e, n_c),\n",
    "    name='V'\n",
    ")\n",
    "Uhh = theano.shared(\n",
    "    get_init_weight(n_hidden_d, n_hidden_d),\n",
    "    name='Uhh'\n",
    ")\n",
    "Uyh = theano.shared(\n",
    "    get_init_weight(n_y, n_hidden_d),\n",
    "    name='Uyh'\n",
    ")\n",
    "Uch = theano.shared(\n",
    "     get_init_weight(n_c, n_hidden_d),\n",
    "    name='Uch'\n",
    ")\n",
    "\n",
    "Uhy = theano.shared(\n",
    "    get_init_weight(n_hidden_d, n_y),\n",
    "    name='Uhy'\n",
    ")\n",
    "Uyy = theano.shared(\n",
    "    get_init_weight(n_y, n_y),\n",
    "    name='Uyy'\n",
    ")\n",
    "Ucy = theano.shared(\n",
    "     get_init_weight(n_c, n_y),\n",
    "    name='Ucy'\n",
    ")\n",
    "def step_d(x, h_tm1, Wx, Wh):\n",
    "    return T.tanh(T.dot(x, Wx) + T.dot(h_tm1, Wh))\n",
    "\n",
    "h_t, updates = theano.scan(\n",
    "    fn=step,\n",
    "    sequences=x,\n",
    "    outputs_info=numpy.zeros(n_hidden_e).astype('float32'),\n",
    "    non_sequences=[Wx, Wh]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##さいごに\n",
    "間違い等あったら、ご連絡ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
