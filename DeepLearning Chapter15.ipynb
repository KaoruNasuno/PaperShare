{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15 Linear Factor Models and Auto-Encoders\n",
    "## メタ情報\n",
    "全部で27ページ。  \n",
    "DAEの章が2つある、auto-encoderの具体的な数式の記述がない等、整っていないところが多い。  \n",
    "auto-encodersは今はあまり注目されていないのかなーと思ってしまった。  \n",
    "だいたい、どの部分にどの分布を仮定するのか、どう正則化項を設けるのか、の話。  \n",
    "\n",
    "## はじめに\n",
    "線形ファクタモデルは観測されない要素$h$で観測変数$x$を線形変換で説明する教師なし学習による生成モデルである。  \n",
    "auto-encodersも教師なしでデータの表現を学習する手法であるが、非線形変換、典型的には$x$から$h$へのフィードフォワードニューラルネットによる変換であることが多い。同時に、線形ファクタモデルのように$h$から$x$の変換も学習する。  \n",
    "したがって、線形ファクタモデルはデコーダのみを明示するが、auto-encoderはエンコーダも明示する。  \n",
    "PCAのように線形ファクタモデルの中には、auto-endoer(線形の)に対応するようなものもあるが、  \n",
    "その他については、encoderは$x$を生成しうる$h$を探索する推論のメカニズムの過程で暗に定義される。\n",
    "\n",
    "<strong>\n",
    "線形ファクタモデルは$h$-> $x$の説明性のみを考慮するが、  \n",
    "auto-encodersは両方向を考慮する。\n",
    "</strong>\n",
    " \n",
    " auto-encodersは長年研究されていたが、最近、注目されるようになった。\n",
    " \n",
    " auto-encodersは単に入力を出力にコピーしようとするフィードフォワードニューラルネットである。  \n",
    " 記号は下記のように利用する。  \n",
    " ${\\bf x}$：入力  \n",
    " $f$：endoder関数  \n",
    " ${\\bf h}=f({\\bf x})$：コード(内部表現)\n",
    " $g$：decoder関数  \n",
    " ${\\bf r}=g({\\bf h})=g(f({\\bf x}))$：出力、reconstruction  \n",
    " $L({\\bf r}, {\\bf x})$：ロス関数\n",
    " \n",
    " ##補足\n",
    " よくある実装は下記の通り。  \n",
    " $\n",
    "{\\bf h}=\\phi( W_1{\\bf x} + b_1)\\\\\n",
    "{\\bf r}=\\phi( W_2{\\bf h} + b_2)\n",
    " $  \n",
    " tied weight制約を設けるのであれば、\n",
    " $W_1 = W_2$\n",
    " \n",
    "目次  \n",
    "1. Regularized Auto-Encoders  \n",
    "2. Denoising Auto-Encoders  \n",
    "3. Representational Power, Layer Size and Depth  \n",
    "4. Reconstruction Distribution  \n",
    "5. Linear Factor Models  \n",
    "6. Probabilistic PCA and Factor Analysis  \n",
    "7. Reconstruction Error as Log-Likelihood  \n",
    "8. Sparse Representations  \n",
    "9. Denoising Auto-Encoders  \n",
    "10. Contractive Auto-Encoders  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Regularized Auto-Encoders  \n",
    "<img src='files/auto-encoders_bottleneck.png' width=\"600px\"/>\n",
    "入力をそのまま出力するモデルでは、いい表現が得られないので、  \n",
    "20世紀では、これを防ぐ為に${\\bf h}$の次元数を${\\bf x}$の次元数よりも少なくしていた。  \n",
    "次元数を少なくするものを_undercomplete_といい、それ以外を_overcomplete_という。  \n",
    "${\\bf h}$の次元を少なくする**bottleneck**制約を利用しないovercompleteの場合、そのまま学習すると恒等関数が学習されてしまうが、  \n",
    "別途制約を設けることで良い表現が学習できる。  \n",
    "\n",
    "### 表現やその派生のスパース性について\n",
    "高次元特徴量が抽出できたとしても，ほとんどの$h$が$0$の場合は有効なlocal dimensionaltyがより小さい可能性がある。  \n",
    "\n",
    "* sparse coding  \n",
    "${\\bf h}=f({\\bf x}) = \\arg \\min L(g({\\bf h}), {\\bf x})) + \\lambda\\Omega({\\bf h})$  \n",
    "とするモデル。  \n",
    "例えば、多くが0か0に近い値となるスパース性を達成したいのであれば、  \n",
    "L1正則化$\\Omega({\\bf h}) = |{\\bf h}|$を利用する。   \n",
    "\n",
    "* PSD(predictive sparse decomposition)  \n",
    "後述。\n",
    "\n",
    "* sparse auto-encoders  \n",
    "他の正則化項として、Student-t penalty、KL-divergence penaltyなどがある。  \n",
    "\n",
    "* contractive autoencoders  \n",
    "$\\Omega({\\bf h})$の制約を設ける。\n",
    "$f$の出力(${\\bf h}$)が収縮するので、contractive autoencodersという。\n",
    "\n",
    "\n",
    "### ノイズや欠損へのロバスト性について\n",
    "auto-encoderは欠損やノイズのないものとして入力の完全な再構築を行おうとするものであり、   \n",
    "ノイズや欠損があると恒等関数を学習することはできない。  \n",
    "したがって、こういった場合に最適な再構築を行う為には、データ分布の構造を捉える必要があり、  \n",
    "こうしたauto-encodersをdenoising auto-encodersとよぶ。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Denoising Auto-Encoders  \n",
    "### denoising auto-encodersとcontractive auto-encodersには強いつながりがある。 \n",
    "(説明が中途半端でロジックが通っていない、、)  \n",
    "ガウス分布に従う小さなノイズを入力に加えるDAEのreconstruction errorは、   \n",
    "reconstruction関数におけるcontractive penaltyと等価である。   \n",
    "言い換えると、${\\bf x}$と${\\bf x}+{\\bf \\epsilon}$を同じ${\\bf x}$に返す為には、全方向の${\\bf \\epsilon}$の変化に対して敏感でないことが期待される。  \n",
    "\n",
    "### 表現のPriorによる制約\n",
    "表現に制約を加えるという概念を一般化する方法として、  \n",
    "$−logP({\\bf h})$を加えるという方法がある。\n",
    "単一の分布に従う表現か、元々のデータよりも単純な表現を学習することがモチベーションであり、  \n",
    "${\\bf h}$をよりシンプルにする(少ないパタンで表現される)>>$−logP({\\bf h})$を下げればいい、という考えによる方法である。  \n",
    "変分auto-encodersでは、ちゃんと、こういったのが組み込まれているらしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Representational Power, Layer Size and Depth  \n",
    "### 層の数\n",
    "これまでの議論には、多層化できないという話はでていないが、単層のautoencodersの事例が多い。  \n",
    "### 理由1：Universal Approximation Theory\n",
    "十分大きな数のUnit数をもつ単層の隠れ層のニューラルネットは指定の精度でどんな関数も近似できる。  \n",
    "これは、Overcompleteを正当化させる理由の一つ。  \n",
    "PCAはundercompleteの事例の一つ(ここで、取り上げる必要あったのか？)。\n",
    "\n",
    "### 理由2：Layerwiseに学習させる方法がメジャー\n",
    "deep auto-encodersでは、Layerwiseに学習し重ねてinitializationする方法でうまくいっており、  \n",
    "議論の中心が単層の学習方法についてであることが多いため。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reconstruction Distribution  \n",
    "これまでの部分は、RMSEによるreconstruction errorを念頭に置いていたが、  \n",
    "必ずしもMSEが適切というわけではない。  \n",
    "例えば、  \n",
    "${\\bf x}$が離散値の場合や、  \n",
    "$P({\\bf x}|{\\bf h})$がガウス分で近似できない場合など。  \n",
    "通常では、  \n",
    "${\\bf x}$が上下界のない実数値の場合はガウス分布を仮定し、  \n",
    "${\\bf x}$が2値の場合はベルヌーイ分布を仮定し、  \n",
    "モデル化することが多いが、  \n",
    "他の混合分布を仮定しても良い。\n",
    "\n",
    "<img src='files/reconstruction_distribution.png' width=\"600px\"/>\n",
    "\n",
    "decoding function  $g({\\bf h})$の概念はdecoding distribution  $P({\\bf x}|{\\bf h})$に一般化できる。  \n",
    "同様に、  \n",
    "encoding function  $f({\\bf x})$の概念はencoding distribution  $P({\\bf h}|{\\bf x})$に一般化できる。   \n",
    "こうすることで、潜在変数のように考えることで、${\\bf h}$の段階でノイズが含まれている事実を捉えることができる。\n",
    "\n",
    "確率的encoderや確率的decoderがRBMにはあるが、詳細は20章にて、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Linear Factor Models  \n",
    "$\n",
    "{\\bf h} \\sim P({\\bf h})\\\\\n",
    "{\\bf x}  = {\\bf W} {\\bf h}  + {\\bf b}  + noise\n",
    "$  \n",
    "ノイズはガウス分布に従う対角のものを使うことが多い。   \n",
    "\n",
    "<img src='files/linear_factor_models.png' width=\"600px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Probabilistic PCA and Factor Analysis  \n",
    "確率的PCAやfactor analysis、他のlinear factor モデルは、上記の数式の特別な事例。  \n",
    "\n",
    "factor analysisでは、  \n",
    "${\\bf h} \\sim N(0,{\\bf I})$  \n",
    "のように、潜在変数のpriorは単に分散1のガウス分布とされ、\n",
    "$x$は互いに独立であり、  \n",
    "ノイズ対角分散ガウス分布から生成されると仮定されている。  \n",
    "\n",
    "潜在変数の役割は、異なる観測データにおける$x$の違いを捉えることである。  \n",
    "${\\bf x} $は単にガウス分布のランダム変数であることが簡単に示すことができる。  \n",
    "${\\bf x}  \\sim N({\\bf b} , {\\bf WW}  +\\psi)$  \n",
    "ただし、$\\psi=diag ({\\bf \\sigma})$のような共分散行列。  \n",
    "\n",
    "確率的PCAについてでは、\n",
    "\n",
    "共分散が$\\sigma$  が全変数で同じだと仮定して、  \n",
    "${\\bf x}  \\sim N({\\bf b} , {\\bf WW}  +\\sigma{\\bf I})$    \n",
    "とするか、全く等価に  \n",
    "${\\bf x}  = {\\bf Wh}  + {\\bf b} + \\sigma z$    \n",
    "ただし、$z  \\sim N(0, {\\bf I})$    \n",
    "のように表現するか。  \n",
    "$\\sigma \\rightarrow  0$の時、確率的PCAはPCAになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reconstruction Error as Log-Likelihood  \n",
    "誤差関数は二乗誤差である必要はない．  \n",
    "Negative Log likelihoodもつかえる。  \n",
    "ロス関数は下記の通り。\n",
    "$L = − \\log P ( {\\bf x}   |   {\\bf   h  } )$  \n",
    "ロス関数は、基本的にデータ${\\bf x}$の特徴に合わせる。  \n",
    "実数の上下界がないデータであれば二乗誤差で、  \n",
    "2値であればベルヌーイ分布を仮定してcross-entropyを選択するのが妥当。 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sparse Representations  \n",
    "### Sparse coding\n",
    "auto-encoderじゃない。  \n",
    "${\\bf x}$を説明する${\\bf h}$をどうやってつくるかの話が中心？？。   \n",
    "${\\bf h}=f({\\bf x}) = \\arg \\min L(g({\\bf h}), {\\bf x})) + \\lambda\\Omega({\\bf h})$  \n",
    "\n",
    "### Sparse auto-encoders\n",
    "$L = - \\log P( {\\bf x} |g({\\bf h})) + \\lambda\\Omega({\\bf h})$  \n",
    "例えば、L1正則化して、0か0に近い値を増やしたい場合は、  \n",
    "$\\Omega({\\bf h})= \\lambda |{\\bf h}|$  \n",
    "\n",
    "ターゲットをしてしたい場合(例えば$\\rho=0.05$)は、   \n",
    "$\\Omega({\\bf h})= \\rho \\log h + (1 - \\rho) \\log (1-h)$  \n",
    "\n",
    "### Predictive Sparse Decomposition\n",
    "物体認識の領域で利用されるpretrainng手法らしい。  \n",
    "TODO：説明加筆。  \n",
    "双方向の誤差を同時に最小化しつつ、$h$のノルムを小さくするように学習。  \n",
    "$L=argmin(   \\|{\\bf x}−g({\\bf h})\\| +\\lambda |{\\bf h}| +\\gamma \\|{\\bf h}−f({\\bf x})\\|)$  \n",
    "最初の2項はsparse codingと同じ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Denoising Auto-Encoders  \n",
    "<img src='files/dae1.png' width=\"600px\"/>\n",
    "1. 訓練データから${\\bf x}$をサンプリング。\n",
    "2. ${\\bf x}$にノイズを加えた${\\bf \\tilde{x}}$を  \n",
    "条件付き分布$C(\\bf {\\tilde{x}}|{\\bf x})$よりサンプリング。\n",
    "3. $({\\bf x}, {\\bf \\tilde{x}})$を訓練データとして、  \n",
    "auto-encoder reconstruction分布\n",
    "$P(\\bf {\\bf x}|{\\tilde{x}}) = P(\\bf {x}|g({\\bf h}))$（ただし、${\\bf h} = f({\\bf \\tilde{x}})$）\n",
    "を推定。\n",
    "\n",
    "DAEは平均二乗誤差$\\|g(f({\\bf \\tilde{x}})) - {\\bf x}\\|$を最小化するように学習するので、   \n",
    "多様体に対して、直交するような形で学習する。  \n",
    "DAEの学習の基準が勾配場を推定するベクトル場$(g(f({\\bf \\tilde{x}})) - {\\bf x})$の学習を行うようになっている。\n",
    "<img src='files/dae2.png' width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Contractive Auto-Encoders  \n",
    "コードに$\\Omega({\\bf h})$の制約を設ける。\n",
    "$f$の出力${\\bf h}$が収縮するので、contractive autoencodersという。\n",
    "\n",
    "$$\\Omega({\\bf h}) = \\frac{\\delta f({\\bf x})}{\\delta {\\bf x}}$$\n",
    "ヤコビアンの行列?の2乗ノルムの和など。  \n",
    "データの復元に必要でないほとんどの次元で小さな値を取るように学習する。\n",
    "\n",
    "<img src='files/CAE1.png' width=\"600px\"/>\n",
    "利点：多様体に直交する方向に対してより普遍性の高い表現を学習できる。  \n",
    "理由：他のautoencodersより少ない次元に表現の感度を集中させようとする為、\n",
    "<img src='files/CAE2.png' width=\"600px\"/>\n",
    "両方とも、ローカルな接線はとれているが、\n",
    "local PCAはよりグローバルないい接線方向の抽出ができていない。  \n",
    "\n",
    "practical issue：\n",
    "* deep CAEを学習させる場合はlayer-wiseに学習させる。  \n",
    "* tied weightつけないと、contraction penaltyがききにくい。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
