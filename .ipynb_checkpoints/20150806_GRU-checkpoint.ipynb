{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文輪読\n",
    "那須野薫(Kaoru Nasuno)/ 東京大学松尾研究室(Matsuo Lab., the University of Tokyo)\n",
    "\n",
    "##メタ情報\n",
    "タイトル：Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation  \n",
    "著者：Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y.   \n",
    "公開：2014  \n",
    "リンク： [arXiv](http://arxiv.org/abs/1406.1078)  \n",
    "被引用件数：84  \n",
    "\n",
    "## Abstract\n",
    "この論文では、2つのRNNからなるRNN Encoder-Decoderという新しいニューラルネットワークを提案する。  \n",
    "ひとつのRNNがシンボルのシーケンスを固定長のベクトル表現にエンコードし、もうひとつがその表現をシンボルのシーケンスにデコードするというもの。  \n",
    "提案モデルのエンコーダとデコーダはソースシーケンスをgivenとしたときのターゲットシーケンスの条件付き確率を最大化するようにひとつのネットワークとして学習させる。  \n",
    "既存のlog-linearモデルの素性にRNN Encoder-Decoderにより算出されたフレーズペアの条件付き確率を利用することで、統計的機械翻訳システムの性能が向上することが実験的に示される。  \n",
    "定性的には、提案モデルが言語フレーズの意味的・文法的に意味のある表現を学習することを示す。\n",
    "\n",
    "\n",
    "##選択理由\n",
    "#### Gated Recurrent Unitが気になっていた。\n",
    "- 最近RNNをいじっている。\n",
    "- LSTMよりマトリックス変数が少ないが精度が同程度？らしい。\n",
    "- 数式の理解と実装が簡単に見える。\n",
    "\n",
    "#### 可変長データを固定長に直し、さらに可変長にする方法を提案している。\n",
    "\n",
    "#### 統計的機械翻訳に特別強い興味がある、というわけではない。\n",
    "\n",
    "\n",
    "## 目次\n",
    "- RNN\n",
    "- RNN Encoder-Decoder\n",
    "- GRU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "<img src='files/RNN2.png' width=\"500px\"/>\n",
    "$$h_{<t>} = f(h_{<t-1>}, x_t)$$  \n",
    "\n",
    "例えば、  \n",
    "$$h_{<t>} = tanh(W_xx_t + W_hh_{<t-1>})$$  \n",
    "$W_h, W_x$：パラメタ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "rng = numpy.random.RandomState(1234)\n",
    "x = T.fmatrix('x')\n",
    "n_x = 10\n",
    "n_hidden_e =10\n",
    "def get_init_weight(n_in, n_out):\n",
    "    return rng.uniform(\n",
    "        low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "        high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "        size=(n_in, n_out),\n",
    "    ).astype('float32')\n",
    "    \n",
    "Wh = theano.shared(\n",
    "    get_init_weight(n_hidden_e, n_hidden_e),\n",
    "    name='Wh'\n",
    ")\n",
    "\n",
    "Wx = theano.shared(\n",
    "    get_init_weight(n_x, n_hidden_e),\n",
    "    name='Wx'\n",
    ")\n",
    "\n",
    "def step_e(x, h_tm1, Wx, Wh):\n",
    "    return T.tanh(T.dot(x, Wx) + T.dot(h_tm1, Wh))\n",
    "\n",
    "h_t_e, _ = theano.scan(\n",
    "    fn=step_e,\n",
    "    sequences=x,\n",
    "    outputs_info=numpy.zeros(n_hidden_e).astype('float32'),\n",
    "    non_sequences=[Wx, Wh]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Encoder-Decoder\n",
    "<img src='files/RNN_Encoder-Decoder.png' width=\"400px\"/>\n",
    "$$論文より引用$$\n",
    "任意の長さのデータを固定長に変換して、さらに任意の長さに変換するRNN。  \n",
    "終了記号(=$x_T$)を読み込んだあとのhidden stateが全体の入力のサマリー(=$c$)になっている。  \n",
    "エンコーダ部分は普通のRNNと同じ。  \n",
    "デコーダ部分は$y_t$も$h'_{<t>}$も$y_{t-1}$の条件づけられる。  \n",
    "$$h'_{<t>}=f(h'_{<t-1>}, y_{t-1}, c)$$  \n",
    "$$y_{t}=f(h'_{<t>}, y_{t-1}, c)$$  \n",
    "学習では下記の$\\theta$を求める。  \n",
    "$$\\max_{\\theta}\\frac{1}{N}\\sum_{n=1}^Nlogp_{\\theta}(y_n|x_n)$$\n",
    "\n",
    "具体例としては、下記のものが考えられる。  \n",
    "(実際には、GRUと組み合わせているので、下記とは異なる。)\n",
    "$$c = tanh(Vh_{T})$$  \n",
    "$$h'_{<t>} = tanh(U_{hh}h'_{<t-1>} + U_{yh}y_{t-1} + U_{ch}c)$$  \n",
    "$$y_t = tanh(U_{hy}h'_{<t>} + U_{yy}y_{t-1} + U_{cy}c)$$  \n",
    "$V, U_{hh}, U_{yh}, U_{ch}, U_{hy}, U_{yy}, U_{cy}$：パラメタ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_len = T.iscalar('y_len')\n",
    "n_hidden_d = 10\n",
    "n_c = 10\n",
    "n_y = 10\n",
    "V = theano.shared(\n",
    "    get_init_weight(n_hidden_e, n_c),\n",
    "    name='V'\n",
    ")\n",
    "Uhh = theano.shared(\n",
    "    get_init_weight(n_hidden_d, n_hidden_d),\n",
    "    name='Uhh'\n",
    ")\n",
    "Uyh = theano.shared(\n",
    "    get_init_weight(n_y, n_hidden_d),\n",
    "    name='Uyh'\n",
    ")\n",
    "Uch = theano.shared(\n",
    "     get_init_weight(n_c, n_hidden_d),\n",
    "    name='Uch'\n",
    ")\n",
    "\n",
    "Uhy = theano.shared(\n",
    "    get_init_weight(n_hidden_d, n_y),\n",
    "    name='Uhy'\n",
    ")\n",
    "Uyy = theano.shared(\n",
    "    get_init_weight(n_y, n_y),\n",
    "    name='Uyy'\n",
    ")\n",
    "Ucy = theano.shared(\n",
    "     get_init_weight(n_c, n_y),\n",
    "    name='Ucy'\n",
    ")\n",
    "\n",
    "c = T.dot(h_t_e[-1], V)\n",
    "\n",
    "def step_d(h_tm1, y_tm1, Uhh, Uyh, Uch, Uhy, Uyy, Ucy, c):\n",
    "    return [\n",
    "        T.tanh(T.dot(h_tm1, Uhh) + T.dot(y_tm1, Uyh) + T.dot(c, Uch)),\n",
    "        T.tanh(T.dot(T.tanh(T.dot(h_tm1, Uhh) + T.dot(y_tm1, Uyh) + T.dot(c, Uch)), Uhy) + T.dot(Uyy, y_tm1) + T.dot(Ucy, c))\n",
    "    ]\n",
    "\n",
    "results, _ = theano.scan(\n",
    "    fn=step_d,\n",
    "    outputs_info=[numpy.zeros(n_hidden_d).astype('float32'), numpy.zeros(n_y).astype('float32')],\n",
    "    non_sequences=[Uhh, Uyh, Uch, Uhy, Uyy, Ucy, c],\n",
    "    n_steps=y_len\n",
    ")\n",
    "h_t_d, y = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## GRU\n",
    "<img src='files/GRU.png' width=\"400px\"/>\n",
    "$$論文より引用$$\n",
    "Gated Recurrent Unit。  \n",
    "LSTMのように、長期的な表現と短期的な表現を捉える為に提案されたactivation function。  \n",
    "ただ、LSTMよりもマトリックス変数が少なく、同程度？の性能が出ている。  \n",
    "なんとなく、こちらの方が良さそうである。  \n",
    "\n",
    "$r$がreset gate(LSTMでのforget gateのようなもの)で、  \n",
    "$z$がupdate gate(LSTMでのmemory cellのようなもの)。  \n",
    "$r$が0に近いと、前のhidden stateを無視して現在のinputのみを考慮する形になる(?)。\n",
    "\n",
    "$$r_j = \\sigma([W_rx]_j + [U_rh_{<t-1>}]_j)$$  \n",
    "$$z_j = \\sigma([W_zx]_j + [U_zh_{<t-1>}]_j)$$  \n",
    "$$h_j^{t} = z_jh_j^{<t-1>} + (1 - z_j) \\tilde{h}_j^{<t>}$$  \n",
    "$$\\tilde{h}_j^{<t>} = tanh([Wx]_j + [U(r \\odot h_{<t-1>})]_j)$$  \n",
    "$W_r, U_r, W_z, U_z, W, U$：パラメタ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験や結論などのそれ以降の内容\n",
    "統計的機械翻訳に興味ある方は自分で確認してください。  \n",
    "ざっと見た感じ、いろいろ拡張性あるので、他分野への応用を検討したい方には勧めます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##まちがいがあったら、おしえてください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
