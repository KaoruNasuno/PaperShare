{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文輪読\n",
    "那須野薫(Kaoru Nasuno)/ 東京大学松尾研究室(Matsuo Lab., the University of Tokyo)\n",
    "\n",
    "## メタ情報\n",
    "タイトル：End-to-End Training of Deep Visuomotor Policies  \n",
    "著者：Sergey Levine, Chelsea Finn, Trevor Darrell, Pieter Abbeel  \n",
    "著者所属：UCB  \n",
    "公開：2015年12月  \n",
    "リンク： [arXiv](http://arxiv.org/abs/1504.00702)  \n",
    "ページ数：34   \n",
    "\n",
    "\n",
    "\n",
    "## Abstract\n",
    "\n",
    "policy searchの手法はロボットの幅広いタスクの制御方策の学習を可能とするが、  \n",
    "policy searchの実応用はしばしば、認識や状態推定や低レベルの制御に手作りの要素を必要とする。  \n",
    "この論文では、  \n",
    "我々は  \n",
    "「認知系と制御系を結合してEnd-to-Endで学習させることで、それぞれの要素を分けて学習させるより、いい性能を得られるか？」  \n",
    "という問いに答えるつもりである。  \n",
    "この問いに答えるために、  \n",
    "我々は観測された生画像を直接ロボットモータのトルク(回転モーメント)にマッピングする方策を学習するのに利用できる手法を開発する。  \n",
    "その方策は7層92,000パラメタのCNNにより表現され、  \n",
    "partially observed guided policy search methodというpolicy searchを教師あり学習に変換する手法により学習する。  \n",
    "この時、教師情報は単純なtrajectory-centric reinforcement learning methodにより与えられる。  \n",
    "我々は、  \n",
    "キャップをボトルに回しながらはめる、  \n",
    "という動作など視覚と制御の緊密な整合性を必要とするような幅広い実世界の操作に対して、我々の手法を評価し、  \n",
    "また、\n",
    "既存のpolicy searchの方法とのシミュレーションされた比較も提示する。\n",
    "\n",
    "\n",
    "\n",
    "## 選択理由\n",
    "\n",
    "#### ロボット制御を少ないサンプルでRLにより学習させる手法を紹介している。  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 発表の方針\n",
    "ページ数が多いので大事なところだけ説明します。    \n",
    "特に、背景/課題とそれを解決する手法を理解できることを意識しております。  \n",
    "説明していないところも少なくないので、わからなかったり疑問に思ったりしたところがあれば、  \n",
    "途中でもいいので聞いてください。\n",
    "\n",
    "## 前知識\n",
    "### Reinforcement Learning\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning#Algorithms_for_control_learning)によると、、、  \n",
    "少なくとも4つの制御学習アルゴリズムがある。\n",
    "- Criterion of optimality\n",
    "- Bruto force\n",
    "- Value function approaches\n",
    "- Direct policy search\n",
    "\n",
    "今回はおそらく、Direct policy searchの話。\n",
    "\n",
    "### ロボット上のDNN\n",
    "ロボット制御のように、実世界の知覚運動方策にDNNを使う場合は、様々な課題がある。  \n",
    "- DNNを十分学習させるだけの操作データが得られない。  \n",
    "- ロボットセンサーからの観測データは系全体の状態を含んでいるとは限らない。例えば、完了状態は画像から類推されるべき。  \n",
    "\n",
    "\n",
    "### Policy search\n",
    "policy $\\pi_\\theta({\\bf u}_t|{\\bf o}_t)$の$\\theta$を最適化すること。  \n",
    "${\\bf u}_t$：時刻$t $の行動。  \n",
    "${\\bf o}_t$：時刻$t $の観測。  カメラ画像やロボットの設定など。  \n",
    "$\\theta$：パラメタ。  \n",
    "\n",
    "\n",
    "\n",
    "### Guided policy search\n",
    "Guided policy searchはpolicy searchを教師あり学習に変換する。  \n",
    "効率的でモデルフリーな軌道最適化手続きを利用することで、訓練データを繰り返し構築している。  \n",
    "\n",
    "###  Partially Observed Guided Policy Search (Guided policy search under unknown dynamics)\n",
    "特に、報告の手法は、\n",
    "訓練時には系全体の状態が観測できるが、テスト時には一部しか観測できないというものである。  \n",
    "ほとんどのタスクにおいて、  \n",
    "すべての状態を与えるためには、訓練中のそれぞれの試行においてすべてのオブジェクトをいくつかの所定の位置に置いておく必要がある。  \n",
    "テスト時には、学習したCNNの方策は新しい、未知の設定に対応することができ、全体の状態はもう必要ない。  \n",
    "教師あり学習で方策が学習されるため、学習にはSGDのような一般的な手法を利用出来る。  \n",
    "数十分で学習する。  \n",
    "\n",
    "訓練データは未知のダイナミクスにおける操作の軌道中心強化学習法(trajectory-centric reinforcement learning methods that operate under unknown dynamics)で作られる。\n",
    "軌道関数と方策関数を[軌道関数=方策関数]という条件下で、ロス関数が最小となるように学習させる。  \n",
    "\n",
    "### BADMM(Bregman alternating directions  method of multipliers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 内容の目次\n",
    "1. Introduction\n",
    "2. Related Work\n",
    "- Overview\n",
    "- Partially Observed Guided Policy Search\n",
    "- End-to-End Visuomotor Policies(省略)\n",
    "- Experimental Evaluation(省略)\n",
    "- Discussion and Future Work\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "<img src='files/img/visuomotor/fig1.png' width=\"900px\"/>\n",
    "\n",
    "----\n",
    "ロボットはすごいことができる。  \n",
    "しかし、無数の操作に対して感覚と制御のソフトウェアを設計するのは、それが基本的なタスクでも長ったらしいプロセスになってしまう。  \n",
    "それを解決するために、policy searchが開発された。  \n",
    "しかし、policy searchの有用性もまだまだ限定的である。\n",
    "\n",
    "----\n",
    "\n",
    "この論文では、  \n",
    "我々は  \n",
    "「認知系と制御系を結合してEnd-to-Endで学習させることで、それぞれの要素を分けて学習させるより、効果的な感覚運動制御のための方策を獲得できるか？」  \n",
    "という問いに答えるつもりである。  \n",
    "この問いに答えるために、  DNNを使う。\n",
    "特に、CNNはCVですごい性能を発揮している。  \n",
    "しかし、実世界の感覚動作の方策へのDNNの活用は限定的である。  \n",
    "DNNがうまくいっているのは、だいたいがデータ数や結果が直接わかるからであるが、ロボット制御ではこれらは得られない。  \n",
    "制御という点では、\n",
    "ロボットセンサーからの観測データは系全体の状態を含んでいるとは限らない。例えば、完了状態は画像から類推されるべきである。  \n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "これらを解決するために、  \n",
    "観測された生画像を直接ロボットモータのトルク(回転モーメント)にマッピングする方策を学習するのに利用できる手法を開発する。  \n",
    "その方策は7層92,000パラメタのCNNにより表現され、  \n",
    "partially observed guided policy search methodというpolicy searchを教師あり学習に変換する手法により学習する。  \n",
    "訓練時には系全体の状態が観測できるが、テスト時には一部しか観測できないというものである。  \n",
    "ほとんどのタスクにおいて、  \n",
    "すべての状態を与えるためには、訓練中のそれぞれの試行においてすべてのオブジェクトをいくつかの所定の位置に置いておく必要がある。  \n",
    "テスト時には、学習したCNNの方策は新しい、未知の設定に対応することができ、全体の状態はもう必要ない。  \n",
    "教師あり学習で方策が学習されるため、学習にはSGDのような一般的な手法を利用出来る。  \n",
    "数十分で学習する。  \n",
    "この時、教師情報は単純なtrajectory-centric reinforcement learning methodにより与えられる。  \n",
    "\n",
    "----\n",
    "PR2やシミュレーションで多くの実験を行った。(実験は省略)  \n",
    "この論文は、著者らがこれまでの国際会議で発表してきた2つの論文を基にして、入力に画像を加えるように拡張したもの。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Related Work\n",
    "強化学習とpolicy searchはロボットに適用されてきた。  \n",
    "しかしこれらはロボット制御パイプの一部のみへの適用であった。   \n",
    "著者らの手法はEnd-to-Endでやる。   \n",
    "\n",
    "----\n",
    "CNNを使う。\n",
    "しかし、位置情報が大事なのでpoolingは使わない。  \n",
    "これまでのロボットへのDL応用は直接制御まで考えていなかった。  \n",
    "\n",
    "----\n",
    "普通のCVと異なり、ロボット制御のはいろいろ制約がある。  \n",
    "微分できないし、計算が安定しないから、ダイナミクスと画像の構成へのBPへの適用は現実的でない。  \n",
    "高次元Nの実世界のRLへの適用可能性も限定的。  \n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Overview\n",
    "<img src='files/img/visuomotor/fig2.png' width=\"900px\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "著者らの手法の目的は、\n",
    "方策$\\pi_\\theta({\\bf u}_t|{\\bf o}_t)$\n",
    "を学習すること。  \n",
    "${\\bf u}_t$：時刻$t $の行動。  \n",
    "${\\bf o}_t$：時刻$t $の観測。  カメラ画像やロボットの設定など。  \n",
    "$\\theta$：パラメタ。  \n",
    "\n",
    "パラメタ$\\theta$はコスト関数$l({\\bf x}_t, {\\bf u}_t)$を固定長のエピソード全体に対して最小化するように学習させる。  \n",
    "行動である${\\bf u}_t$はモータのトルクを、  \n",
    "状態である${\\bf x}_t$は例えば、目的物の位置を含む系の情報。  \n",
    "後者は方策からは直接観測できず、カメラ画像などから推論されるべきもの。  \n",
    "これが、部分的に観測可能なタスクとしている理由。  \n",
    "\n",
    "方策$\\pi_\\theta({\\bf u}_t|{\\bf o}_t)$は条件付き正規分布で、平均を生画像を入力とするCNNで与える。  \n",
    "CNNを学習させるために、guided policy searchを拡張する。  \n",
    "guided policy searchでは方策は高次元近似が可能な教師あり学習で学習される。  \n",
    "訓練データを作成するために、trajectory-centric reinforcement learning method という、多くの初期状態からのいい軌道を見つける手法を利用する。\n",
    "このフェーズでは、系のダイナミクスの情報は不要だが、${\\bf x}_t$は必要。 \n",
    "\n",
    "汎化性能に関するさまざまな問題をBADMMを利用することで解決。  \n",
    "BADMMにより、軌道を方策に適合するようにし、方策を軌道に合致するように最適化し、また、軌道をコスト最小化と方策に合致するように最適化させる。  \n",
    "収束時には、方策と軌道は同じ状態分布となる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Partially Observed Guided Policy Search\n",
    "<img src='files/img/visuomotor/ins1.png' width=\"300px\"/>\n",
    "\n",
    "guided policy searchはpolicy searchを教師あり学習に変換する。この時、教師データは単純なtrajectory-centric guilded　アルゴリズムにより生成される。  \n",
    "軌道フェーズでは、ガウス軌道分布$p_i(\\tau)$を生成するが、これは線系フィードバックを伴う平均軌道に相当する。  \n",
    "各$p_i(\\tau)$は特定の初期状態に続いて起こる。  \n",
    "例えば、ボトルにキャップをはめるタスクであれば、初期状態はボトルの位置など。  \n",
    "\n",
    "\n",
    "この章では、guided policy searchが部分観測タスクをどのように解くのかを説明する。  \n",
    "このタスクでは、  \n",
    "方策$\\pi_\\theta({\\bf u}_t|{\\bf o}_t)$いは系全体${\\bf x}_t$の観測だけ提供され、  ダイナミクスは未知である。  \n",
    "手法は図の流れになっている。  \n",
    "\n",
    "----\n",
    "外側のループは、物理系の初期状態のサンプルを生成させる。  \n",
    "そのサンプルは、\n",
    "軌道最適化のためダイナミクス$pi_({\\bf u}_t|{\\bf o}_t)$にフィットするように利用され、  \n",
    "方策の学習の訓練データに活用される。  \n",
    "\n",
    "----\n",
    "内側のループは\n",
    "各$p_i$の最適化とこれら軌道分布へ合致するような方策の最適化を交互に繰り返す。  \n",
    "方策は、系全体${\\bf x}_t$の情報ではなく、観測${\\bf o}_t$から描く軌道に対しての行動を予測する。\n",
    "こうすることで、この方策はテスト時にも部分観測を処理できる。  \n",
    "この交互に最適化するのはBADMMアルゴリズムのインスタントして体系化でき、\n",
    "このアルゴリズムは、軌道分布と方策分布が同じ状態分布を持つような解に収束する。  \n",
    "また、こうすることで、greedyな教師あり学習によりいい感じに方策を学習できる。\n",
    "\n",
    "### 4.1 Algorithm Derivation\n",
    "policy search methodは期待コスト  \n",
    "$E_{\\pi_\\theta}[l(\\tau)]$を最小化する。  \n",
    "ここでは、$\\tau = \\{{\\bf x}_1, {\\bf u}_1, \\cdots, {\\bf x}_T, {\\bf u}_T\\}$は軌道であり、  \n",
    "$l(\\tau)= \\sum^T_{t=1} l({\\bf x}_t, {\\bf u}_t)$は1エピソードのコスト関数である。  \n",
    "\n",
    "系全体が見えている場合と異なり、  \n",
    "部分観測タスクでは、  \n",
    "我々が知っているのは$\\pi_\\theta({\\bf u}_t|{\\bf o}_t)$だけで、  \n",
    "$\\pi_\\theta({\\bf u}_t|{\\bf x}_t)$はわからないが、\n",
    "これは、$\\pi_\\theta({\\bf u}_t|{\\bf x}_t) = \\int \\pi_\\theta({\\bf u}_t|{\\bf o}_t)p({\\bf o}_t|{\\bf x}_t)d{\\bf o}_t$として復元されうる。  \n",
    "最終的な、アルゴリズムでは$p({\\bf o}_t|{\\bf x}_t)$の知識は不要。  \n",
    "制約最適化問題として描くと、\n",
    "<img src='files/img/visuomotor/eq1.png' width=\"400px\"/>\n",
    "\n",
    "ここでは、$p(\\tau)$は軌道分布。    \n",
    "この定式化は、2つの分布を同じとしているので、元々の問題設定と全く同じである。  \n",
    "この問題は、dual decent methodにより解くことができる。  \n",
    "このdual decent methodはそれぞれのprimal variablesについてのラグランジアンの最小化と劣勾配(subgradient)によるラグランジュ乗数のincrementを交互に繰り返し行う。  \n",
    "このdural decent methodの説明(省略)。  \n",
    "\n",
    "\n",
    "### 4.2 Trajectory Optimization under Unknown Dynamics\n",
    "\n",
    " $p(\\tau)$ガウス分布なため、  \n",
    " ダイナミクスとコントローラに相当する$p({\\bf x}_{t+1}|{\\bf x}_t,{\\bf u}_t)$と$p({\\bf u}_t|{\\bf x}_t)$は線形ガウス分布である。  \n",
    " ダイナミクスは環境によって毛一定される。  \n",
    " もしダイナミクスが既知であれば、$p({\\bf u}_t|{\\bf x}_t)$はiterative linear-quadratic regulatorの変形で最適化できる。  \n",
    "unknown dynamicsの場合は、  \n",
    "$p({\\bf x}_{t+1}|{\\bf x}_t,{\\bf u}_t)$を前の繰り返しでの軌道分布($ \\hat{p}(\\tau)$)より収集された軌道サンプルにフィットするさせることができる。  \n",
    "もし、$ \\hat{p}(\\tau)$が$ p(\\tau)$よと大きく異なれば、これらのサンプルは$p({\\bf x}_{t+1}|{\\bf x}_t,{\\bf u}_t)$をよく推定できず、最適化は発散する。  \n",
    "これを回避するため、$ \\hat{p}(\\tau)$と$ p(\\tau)$のKLダイバージェンスにステップサイズ$\\epsilon$で閾値を設ける。  \n",
    "\n",
    "\n",
    "<img src='files/img/visuomotor/eq2.png' width=\"400px\"/>\n",
    "### 4.3 Supervised Policy Optimization\n",
    "パラメタ$\\theta$は式1の制約にしか依らず、\n",
    "いくつか事前分布を仮定することでSGDで学習できる。  \n",
    "例えば、\n",
    "<img src='files/img/visuomotor/eq4.png' width=\"300px\"/>\n",
    "\n",
    "\n",
    "### 4.4 Comparison with Prior Guided Policy Search Methods\n",
    "方策は観測に基づいて学習され、軌道は系全体に基づいて学習されるguided policy search methodを紹介した。   \n",
    "制約最適化問題に基づいた事前分布によるguided policy search手法は提案されているものの、\n",
    "guided policy searchのBADMM定式化は本研究が初である。  \n",
    "$p(\\tau)$と$\\pi_{\\theta}$にKLダイバージェンスで制約を設けたのも初。  \n",
    "\n",
    "ADMMを使ったguided policy searchは提案れているが、この手法は既知の決定論的なダイナミクスが必要で決定的な方策を学習し、しかも、2階微分する必要がある。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. End-to-End Visuomotor Policies\n",
    "- ロボットの部分を含めた全体のアーキテクチャについて。  \n",
    "- ネットワークの学習について。  \n",
    "\n",
    "興味ある方は自分でみてください。\n",
    "## 6. Experimental Evaluation\n",
    "いくつも実験を行っている。   \n",
    "だいたい実験はうまくいっている。  \n",
    "興味ある方は自分でみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7. Discussion and Future Work\n",
    "\n",
    "- 視覚的な障害物への対応\n",
    "- 同時に複数台で学習させる\n",
    "- recurrent policyへの拡張  \n",
    "- 聴覚導入の効果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
