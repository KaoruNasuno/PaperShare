{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文輪読\n",
    "那須野薫(Kaoru Nasuno)/ 東京大学松尾研究室(Matsuo Lab., the University of Tokyo)\n",
    "\n",
    "## メタ情報\n",
    "タイトル：End-to-End Training of Deep Visuomotor Policies  \n",
    "著者：Sergey Levine, Chelsea Finn, Trevor Darrell, Pieter Abbeel  \n",
    "著者所属：UCB  \n",
    "公開：2015年12月  \n",
    "リンク： [arXiv](http://arxiv.org/abs/1504.00702)  \n",
    "ページ数：34   \n",
    "\n",
    "\n",
    "\n",
    "## Abstract\n",
    "\n",
    "policy searchの手法はロボットの幅広いタスクの制御方策の学習を可能とするが、  \n",
    "policy searchの実応用はしばしば、認識や状態推定や低レベルの制御に手作りの要素を必要とする。  \n",
    "この論文では、  \n",
    "我々は  \n",
    "「認知系と制御系を結合してEnd-to-Endで学習させることで、それぞれの要素を分けて学習させるより、いい性能を得られるか？」  \n",
    "という問いに答えるつもりである。  \n",
    "この問いに答えるために、  \n",
    "我々は観測された生画像を直接ロボットモータのトルク(回転モーメント)にマッピングする方策を学習するのに利用できる手法を開発する。  \n",
    "その方策は7層92,000パラメタのCNNにより表現され、  \n",
    "partially observed guided policy search methodというpolicy searchを教師あり学習に変換する手法により学習する。  \n",
    "この時、教師情報は単純なtrajectory-centric reinforcement learning methodにより与えられる。  \n",
    "我々は、  \n",
    "キャップをボトルに回しながらはめる、  \n",
    "という動作など視覚と制御の緊密な整合性を必要とするような幅広い実世界の操作に対して、我々の手法を評価し、  \n",
    "また、\n",
    "既存のpolicy searchの方法とのシミュレーションされた比較も提示する。\n",
    "\n",
    "\n",
    "\n",
    "## 選択理由\n",
    "\n",
    "#### ロボット制御を少ないサンプルでRLにより学習させる手法を紹介している。  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 発表の方針\n",
    "ページ数が多いので大事なところだけ説明します。    \n",
    "特に、背景/課題とそれを解決する手法を理解できることを意識しております。  \n",
    "説明していないところも少なくないので、わからなかったり疑問に思ったりしたところがあれば、  \n",
    "途中でもいいので聞いてください。\n",
    "\n",
    "## 前知識\n",
    "### Reinforcement Learning\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning#Algorithms_for_control_learning)によると、、、  \n",
    "少なくとも4つの制御学習アルゴリズムがある。\n",
    "- Criterion of optimality\n",
    "- Bruto force\n",
    "- Value function approaches\n",
    "- Direct policy search\n",
    "\n",
    "今回はおそらく、Direct policy searchの話。\n",
    "\n",
    "### ロボット上のDNN\n",
    "ロボット制御のように、実世界の知覚運動方策にDNNを使う場合は、様々な課題がある。  \n",
    "- DNNを十分学習させるだけの操作データが得られない。  \n",
    "- ロボットセンサーからの観測データは系全体の状態を含んでいるとは限らない。例えば、完了状態は画像から類推されるべき。  \n",
    "\n",
    "\n",
    "### Policy search\n",
    "policy $\\pi_\\theta({\\bf u}_t|{\\bf o}_t)$の$\\theta$を最適化すること。  \n",
    "${\\bf u}_t$：時刻$t $の行動。  \n",
    "${\\bf o}_t$：時刻$t $の観測。  カメラ画像やロボットの設定など。  \n",
    "$\\theta$：パラメタ。  \n",
    "\n",
    "\n",
    "\n",
    "### Guided policy search\n",
    "Guided policy searchはpolicy searchを教師あり学習に変換する。  \n",
    "効率的でモデルフリーな軌道最適化手続きを利用することで、訓練データを繰り返し構築している。  \n",
    "\n",
    "###  Partially Observed Guided Policy Search (Guided policy search under unknown dynamics)\n",
    "特に、報告の手法は、\n",
    "訓練時には系全体の状態が観測できるが、テスト時には一部しか観測できないというものである。  \n",
    "ほとんどのタスクにおいて、  \n",
    "すべての状態を与えるためには、訓練中のそれぞれの試行においてすべてのオブジェクトをいくつかの所定の位置に置いておく必要がある。  \n",
    "テスト時には、学習したCNNの方策は新しい、未知の設定に対応することができ、全体の状態はもう必要ない。  \n",
    "教師あり学習で方策が学習されるため、学習にはSGDのような一般的な手法を利用出来る。  \n",
    "数十分で学習する。  \n",
    "\n",
    "訓練データは未知のダイナミクスにおける操作の軌道中心強化学習法(trajectory-centric reinforcement learning methods that operate under unknown dynamics)で作られる。\n",
    "軌道関数と方策関数を[軌道関数=方策関数]という条件下で、ロス関数が最小となるように学習させる。  \n",
    "\n",
    "### BADMM(Bregman alternating directions  method of multipliers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 内容の目次\n",
    "1. Introduction\n",
    "2. Related Work\n",
    "- Overview\n",
    "- Partially Observed Guided Policy Search\n",
    "- End-to-End Visuomotor Policies(省略)\n",
    "- Experimental Evaluation(省略)\n",
    "- Discussion and Future Work\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "<img src='files/img/visuomotor/fig1.png' width=\"900px\"/>\n",
    "\n",
    "----\n",
    "ロボットはすごいことができる。  \n",
    "しかし、無数の操作に対して感覚と制御のソフトウェアを設計するのは、それが基本的なタスクでも長ったらしいプロセスになってしまう。  \n",
    "それを解決するために、policy searchが開発された。  \n",
    "しかし、policy searchの有用性もまだまだ限定的である。\n",
    "\n",
    "----\n",
    "\n",
    "この論文では、  \n",
    "我々は  \n",
    "「認知系と制御系を結合してEnd-to-Endで学習させることで、それぞれの要素を分けて学習させるより、効果的な感覚運動制御のための方策を獲得できるか？」  \n",
    "という問いに答えるつもりである。  \n",
    "この問いに答えるために、  DNNを使う。\n",
    "特に、CNNはCVですごい性能を発揮している。  \n",
    "しかし、実世界の感覚動作の方策へのDNNの活用は限定的である。  \n",
    "DNNがうまくいっているのは、だいたいがデータ数や結果が直接わかるからであるが、ロボット制御ではこれらは得られない。  \n",
    "制御という点では、\n",
    "ロボットセンサーからの観測データは系全体の状態を含んでいるとは限らない。例えば、完了状態は画像から類推されるべきである。  \n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "これらを解決するために、  \n",
    "観測された生画像を直接ロボットモータのトルク(回転モーメント)にマッピングする方策を学習するのに利用できる手法を開発する。  \n",
    "その方策は7層92,000パラメタのCNNにより表現され、  \n",
    "partially observed guided policy search methodというpolicy searchを教師あり学習に変換する手法により学習する。  \n",
    "訓練時には系全体の状態が観測できるが、テスト時には一部しか観測できないというものである。  \n",
    "ほとんどのタスクにおいて、  \n",
    "すべての状態を与えるためには、訓練中のそれぞれの試行においてすべてのオブジェクトをいくつかの所定の位置に置いておく必要がある。  \n",
    "テスト時には、学習したCNNの方策は新しい、未知の設定に対応することができ、全体の状態はもう必要ない。  \n",
    "教師あり学習で方策が学習されるため、学習にはSGDのような一般的な手法を利用出来る。  \n",
    "数十分で学習する。  \n",
    "この時、教師情報は単純なtrajectory-centric reinforcement learning methodにより与えられる。  \n",
    "\n",
    "----\n",
    "PR2やシミュレーションで多くの実験を行った。(実験は省略)  \n",
    "この論文は、著者らがこれまでの国際会議で発表してきた2つの論文を基にして、入力に画像を加えるように拡張したもの。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Related Work\n",
    "強化学習とpolicy searchはロボットに適用されてきた。  \n",
    "しかしこれらはロボット制御パイプの一部のみへの適用であった。   \n",
    "著者らの手法はEnd-to-Endでやる。   \n",
    "\n",
    "----\n",
    "CNNを使う。\n",
    "しかし、位置情報が大事なのでpoolingは使わない。  \n",
    "これまでのロボットへのDL応用は直接制御まで考えていなかった。  \n",
    "\n",
    "----\n",
    "普通のCVと異なり、ロボット制御のはいろいろ制約がある。  \n",
    "微分できないし、計算が安定しないから、ダイナミクスと画像の構成へのBPへの適用は現実的でない。  \n",
    "高次元Nの実世界のRLへの適用可能性も限定的。  \n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Overview\n",
    "<img src='files/img/visuomotor/fig2.png' width=\"900px\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "著者らの手法の目的は、\n",
    "方策$\\pi_\\theta({\\bf u}_t|{\\bf o}_t)$\n",
    "を学習すること。  \n",
    "${\\bf u}_t$：時刻$t $の行動。  \n",
    "${\\bf o}_t$：時刻$t $の観測。  カメラ画像やロボットの設定など。  \n",
    "$\\theta$：パラメタ。  \n",
    "\n",
    "パラメタ$\\theta$はコスト関数$l({\\bf x}_t, {\\bf u}_t)$を固定長のエピソード全体に対して最小化するように学習させる。  \n",
    "行動である${\\bf u}_t$はモータのトルクを、  \n",
    "状態である${\\bf x}_t$は例えば、目的物の位置を含む系の情報。  \n",
    "後者は方策からは直接観測できず、カメラ画像などから推論されるべきもの。  \n",
    "これが、部分的に観測可能なタスクとしている理由。  \n",
    "\n",
    "方策$\\pi_\\theta({\\bf u}_t|{\\bf o}_t)$は条件付き正規分布で、平均を生画像を入力とするCNNで与える。  \n",
    "CNNを学習させるために、guided policy searchを拡張する。  \n",
    "guided policy searchでは方策は高次元近似が可能な教師あり学習で学習される。  \n",
    "訓練データを作成するために、trajectory-centric reinforcement learning method という、多くの初期状態からのいい軌道を見つける手法を利用する。\n",
    "このフェーズでは、系のダイナミクスの情報は不要だが、${\\bf x}_t$は必要。 \n",
    "\n",
    "汎化性能に関するさまざまな問題をBADMMを利用することで解決。  \n",
    "BADMMにより、軌道を方策に適合するようにし、方策を軌道に合致するように最適化し、また、軌道をコスト最小化と方策に合致するように最適化させる。  \n",
    "収束時には、方策と軌道は同じ状態分布となる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Partially Observed Guided Policy Search\n",
    "<img src='files/img/visuomotor/ins1.png' width=\"300px\"/>\n",
    "\n",
    "\n",
    "### 4.1 Algorithm Derivation\n",
    "\n",
    "### 4.2 Trajectory Optimization under Unknown Dynamics\n",
    "\n",
    "### 4.3 Supervised Policy Optimization\n",
    "\n",
    "\n",
    "### 4.4 Comparison with Prior Guided Policy Search Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7. Discussion and Future Work\n",
    "\n",
    "- 視覚的な障害物への対応\n",
    "- 同時に複数台で学習させる\n",
    "- recurrent policyへの拡張  \n",
    "- 聴覚導入の効果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
